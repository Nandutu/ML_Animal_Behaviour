---
title: "Pheasant Eggs"
output: html_document
---

# Introduction

The data comprises of 569 eggs and a total of 21 spectral and morphological characteristics,
which describe the appearance of each egg. Eggs have no corresponding label.

**Objective**: *"To determine parentage of clutches"*


# Read, clean and normalise data

We start by reading in the data and remove egg IDs (observations) which were incorrectly 
recorded. Finally, we normalise the data because the range of values across covariates changes
drastically (e.g `mass` varies ~25 - 35 but `h.theta` varies -0.1 - 0.1), else this would 
have a detrimental effect on distance-based algorithms.

```{r, warning=FALSE, message=FALSE}
# Clear workspace
rm(list = setdiff(ls(), lsf.str())) 

# Load all libraries to be used in subsequent analyses
library(knitr)
library(rgl) # plot3d
library(rglwidget) # for 3D plotting
knit_hooks$set(webgl = hook_webgl) # to get interactive 3D plots in HTML mode
library(cluster) # silhouette diagram
library(corrgram) # correlogram
library(dendextend) # to colour dendrogram branches
library(RColorBrewer)

# Read data
data <- read.table(file="data.csv", header=T, sep=",", skip=0, row.names="Egg.ID")
# Clean data 
REMOVE <- c(79, 116, 485) # Egg.IDs to remove 
data <- data[!(row.names(data) %in% REMOVE), ] # include everything apart from REMOVE
# Normalise data (x' = [x - mean(x)] / std(x))
predictorNames <- setdiff(names(data), c("Ordering", "Pen", "File", "File.name"))
xTrain <- data[, predictorNames] # unsupervised learning we don't split into train/test
meanXTrain <- apply(xTrain, 2, mean) # mean of each attribute
sdXTrain <- apply(xTrain, 2, sd) # standard deviation of each attribute
xTrain <- sweep(xTrain, 2, meanXTrain, FUN="-") # remove means
xTrain <- sweep(xTrain, 2, sdXTrain, FUN="/") # normalise by standard deviation
```

# Data exploration

Because we only have 21 variables, we can explore the data using conventional means
such as pair-wise scatter plots and correlogram.

```{r, warning=FALSE, message=FALSE, fig.width=10, fig.height=10}
# Pair-wise scatter plots
pairs(x=xTrain, upper.panel=NULL, cex.labels=1, pch='.', col="grey")
# Compute Pearson correlation coefficients
corrgram(x=xTrain, lower.panel=panel.shade, upper.panel=panel.conf)
```


# Feature extraction using PCA

The strong *linear* relationships across variables of the same theme (spectral or 
morphological) suggest the use of a linear dimensionality reduction framework.
Here, we apply **Principal Component Analysis** (PCA). 

```{r, warning=FALSE, message=FALSE}
pca <- prcomp(x=xTrain, retx=TRUE, center=FALSE, scale.=FALSE) # apply PCA
summary(pca)
varianceExplained <- pca$sdev^2 / sum(pca$sdev^2) # compute explained variance
varianceExplained <- varianceExplained[1:10] # plot only the top 10 PCs
# Plot % variance explained by each principal component (PC)
barplot(100*varianceExplained, las=1, ylab='Variance Explained (%)', 
        names.arg=paste("PC", seq(length(varianceExplained)), sep=""))
# Plot cumulative % variance explained by each principal component (PC)
plot(cumsum(100*varianceExplained), las=1, ylab='Cumulative Variance Explained (%)', 
     xlab="Principal Component (PC)", type="b", pch=19)
# Plot pair-wise scatter plot and pearson corelation coefficient for top 4 PCs 
pairs(x=pca$x[, 1:4], upper.panel=NULL, cex.labels=1, pch='.', col="grey")
corrgram(x=pca$x[, 1:4], lower.panel=panel.shade, upper.panel=panel.conf)
```

We will keep the top 4 components as they explain 96% of the variance in the
data (the pair-wise scatter plot confirms that these components are uncorrelated).
PC1 to PC4 are linear combinations of the original variables, that is:

$$
PC = w_1\mathrm{u} + w_2\mathrm{s} + \ldots + w_{20}\mathrm{Mass} + w_{21}\mathrm{Width.Length.ratio}
$$

We can therefore give the PCs a biological interpretation by inspecting/plotting their 
weights. 

```{r, warning=FALSE, message=FALSE, fig.width=5, fig.height=10, fig.align='center'}
par(mfrow=c(4, 1))
yMin <- min(pca$rotation^2)
yMax <- max(pca$rotation^2)
# Plot and compare weights^2 as sum(weights^2)=1
for (i in seq(4))
{
    barplot(pca$rotation[, i]^2, col="grey", ylim=c(yMin, yMax), las=2, 
            main=paste("PC", i, sep=""), ylab="Weight$^2$")
}
```

The following interpretation can be given (please refer to manuscript):

* PC1 is a measure of eggshell brightness
* PC2 is a measure of eggshell ``greenness"
* PC3 is a measure of egg size
* PC4 is a measure of egg shape

# Clustering

Let us start by visualising the ``new" features, that is, PC1 - PC4, to see if we can
observe some groupings.
From now on we focus on a single pen to illustrate the results. The 
analysis can be easily extended to the rest of the pens. The data points are labelled
by a unique egg ID.

```{r, warning=FALSE, message=FALSE, webgl=TRUE, fig.width=8, fig.height=8, fig.align='center'}
par(mfrow=c(1, 1)) # reset plotting area
# Name features
PC1 <- "Eggshell brightness"
PC2 <- "Eggshell greenness"
PC3 <- "Egg size"
xTrain <- pca$x[data$Pen==1, 1:4] # keep only first 4 PCs and pen=1
# Plot PC1 vs PC2
plot(xTrain[, 1], xTrain[, 2], xlab=PC1, ylab=PC2, type="n")  
text(xTrain[, 1], xTrain[, 2], labels=row.names(xTrain), col="grey")
# Plot PC1 vs PC2 vs PC3
plot3d(xTrain[, 1], xTrain[, 2], xTrain[, 3], xlab=PC1, ylab=PC2, zlab=PC3, type="n")
text3d(xTrain[, 1], xTrain[, 2], xTrain[, 3], row.names(xTrain), col="grey")
```

We know *a priori* that there are three females in each pen and therefore expect three
distinct clusters. These are somewhat visible in the 3D plot, but less in the 2D plot.
Let us apply $k$-means and agglomerative hierarchical clustering, and compare results. 

## $k$-Means

```{r, warning=FALSE, message=FALSE, webgl=TRUE, fig.width=8, fig.height=8, fig.align='center'}
set.seed(101) # for reproducibility
fit <- kmeans(xTrain, centers=3, nstart=50, iter.max=1001)
# Visualise result in 2D
plot(xTrain[, 1], xTrain[, 2], xlab=PC1, ylab=PC2, type="n",  cex.lab=1.4, cex.axis=1.4)  
text(xTrain[, 1], xTrain[, 2], labels=row.names(xTrain), col=fit$cluster, cex=1.4) # egg ID
# Visualise result in 3D
plot3d(xTrain[, 1], xTrain[, 2], xTrain[, 3], xlab=PC1, ylab=PC2, zlab=PC3, type="n")
text3d(xTrain[, 1], xTrain[, 2], xTrain[, 3], row.names(xTrain), col=fit$cluster)
```

### Guessing the number of clusters

There are several scenarios where the number of clusters is not know *a priori* and needs
to be estimated. Here we use silhouette plots to show how one can do so, and to confirm
that in this case $k=3$ is the suggested number of clusters in the data.

```{r, warning=FALSE, message=FALSE, fig.width=10, fig.height=10, fig.align='center'}
set.seed(101) # for reproducibility
colours <- brewer.pal(n=5, name="Set1")
kRange <- seq(from=2, to=5, by=1)
par(mfrow=c(2, 2))
for (k in kRange)
{
    fit <- kmeans(xTrain, centers=k, nstart=50, iter.max=1001)
    silh <- silhouette(x=fit$cluster, dist=daisy(xTrain)^2) 
    plot(silh, main=paste("k=", k), col=colours[1:k])
}
```

## Agglomerative Hierarchical 

For comparison, we cluster the data using agglomerative hierarchical clustering, using
a euclidean distance metric and Ward's minimum variance method.

```{r, warning=FALSE, message=FALSE, fig.width=10, fig.height=8, fig.align='center'}
par(mfrow=c(1, 1)) # reset plot to 1 x 1 
distance <- dist(xTrain, method="euclidean") # distance function = euclidean distance
fit <- hclust(d=distance, method="ward.D2") # Ward's method
dend <- as.dendrogram(fit) # convert hclust to dendrogram class
dend <- color_branches(dend, k=3, col=c(2, 1, 3)) # match k-means colours
plot(dend, ylab="Distance", xlab="Egg ID", cex.lab=1.3, cex.axis=1.3)
```
